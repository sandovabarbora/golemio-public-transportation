# Public Transport Analysis Dashboard ğŸšŒ

A Streamlit-based dashboard for analyzing public transport delays and their relationship with sporting events. The application provides real-time insights, visualizations, and predictions for transport planners and analysts.

## Real-World Impact ğŸ¯

> It's a Friday afternoon in December, and AleÅ¡ is finishing up his work at the office. As a data analyst in Prague and an avid fan of Sparta, he faces a common dilemma: how to maximize his work time without risking a late arrival at the stadium. With tonight's match approaching, he knows that public transportation delays can be unpredictableâ€”especially with potential extra delays caused by increased traffic around LetnÃ¡ during event times and weather conditions. Therefore, AleÅ¡ pulls out the transit analysis dashboard, that he has been using for the entire season, and plans his journey thanks to its extraordinary predictive capabilities.


## Features ğŸŒŸ
- **Real-time Delay Analysis**: Track and visualize current transport delays
- **Event Impact Assessment**: Analyze how sports events affect transport patterns
- **Interactive Maps**: View delay patterns through markers or heatmaps
- **Predictive Analytics**: Get short-term delay predictions with reliability scores

## System Requirements âš™ï¸
- Python 3.9 or higher
- Minimum 8GB RAM
- Docker (optional, for containerized deployment)
- Sufficient storage for data processing (minimum 10GB recommended)

## Quick Start ğŸš€

### Prerequisites
- Python 3.9 or higher
- Azure account with blob storage access
- Golemio API access token

### Installation Options

#### Local Setup
1. Clone the repository:
```bash
git clone https://github.com/yourusername/public-transport-analysis.git
cd public-transport-analysis
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file with your credentials:
```env
parquetAzureTenantID=your_tenant_id
parquetAzureAppID=your_app_id
parquetAzureClientSecret=your_client_secret
parquetStorageName=your_storage_name
X-Access-Token-Golemio=your_golemio_token
```

5. Run the application:
```bash
streamlit run app.py
```

#### Docker Setup
1. Build the Docker image:
```bash
docker build -t transport-analysis .
```

2. Run the container:
```bash
docker run -p 8501:8501 --env-file .env transport-analysis
```

## Important Notes âš ï¸

### Resource Requirements
- Memory usage can peak at 2-3GB during data processing
- Significant CPU usage for real-time calculations
- Regular cache clearing recommended for optimal performance

### Known Limitations
- Currently not deployable to cloud platforms (Heroku, Streamlit Cloud) due to memory requirements
- Large datasets require incremental processing
- Real-time updates may be affected by API rate limits

## Dashboard Overview ğŸ“Š

### 1. Delay Statistics
- Current delay patterns
- Historical trends
- Interactive map visualization
- Time-based filtering

### 2. Event Analysis
- Sports event impact assessment
- Before/after event comparisons
- Pattern identification
- Impact zone mapping

### 3. Delay Predictions
- Short-term delay forecasts
- Reliability scoring
- Pattern-based predictions
- Interactive trend visualization

## Data Sources ğŸ“‚
The dashboard integrates data from multiple sources:
- Transport data from Azure Data Lake - stop times parquet files filtered to Prague 7, provided by Golemio
- Sports event schedules - scraping Sparta Prague matches from Eurofotbal.cz
- Stop information and geography data - Golemio API

## Documentation ğŸ“š
For detailed information, check out the docs folder:
- [Technical Documentation](docs/technical.md)
- [User Guide](docs/user_guide.md)
- [Deployment Guide](docs/deployment.md)
- [Prediction Methodology](docs/prediction.md)
- [Limitations and Future Work](docs/limitations.md)

## Project Structure ğŸ“
```
public-transport-dashboard/
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose setup
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ data/                   # Data files
â”‚   â”œâ”€â”€ letna_stops.csv
â”‚   â””â”€â”€ sparta_matches.csv
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ connectors/        # Data connectors
â”‚   â”œâ”€â”€ models/           # Analysis models
â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â””â”€â”€ views/            # Dashboard views
â””â”€â”€ docs/                  # Documentation
```

## Performance Tips ğŸ’¡
- Use date filters to limit data loading
- Enable caching for faster reloads
- Clear cache regularly for optimal performance
- Process data in smaller chunks
- Update data during off-peak hours

## Troubleshooting ğŸ”§
Common issues and solutions:
1. **Memory Issues**: Reduce date range, clear cache, or restart application
2. **Data loading fails**: Check Azure credentials and connection
3. **Maps not displaying**: Verify internet connection and Folium installation
4. **Prediction errors**: Ensure sufficient historical data is available
5. **Performance issues**: Monitor resource usage, optimize query range

## Future Plans ğŸ”®
- Integration with additional data sources (Waze, FCD, NDIC)
- Enhanced memory optimization
- Cloud deployment solutions
- Advanced analytics features

## Acknowledgments ğŸ™
We gratefully acknowledge the support of various AI tools that assisted with code optimization, as well as our data providers who made this project possible. However, the code base and the main idea still lies in our heads, and our heads only. Despite current technical and resource limitations, our work demonstrates the potential for comprehensive public transport analysis. We remain committed to continuous improvement through resource optimization, feature enhancement, and performance refinement as we work toward a more scalable solution. Moreover, let us thank Golemio Prague Data Platform for providing the data.
---
Made with â¤ï¸ by Barbora and VojtÄ›ch